Код представляет собой веб-краулер, который выполняет параллельные запросы к указанным URL-адресам, ограничивая количество одновременно работающих горутин (максимум 8) и позволяя отменить процесс с помощью сигнала `Ctrl+C`. Ниже приводится объяснение работы каждой функции и ключевых частей программы.

### Функция `crawlWeb`

```go
func crawlWeb(ctx context.Context, urls chan string) chan string {
	out := make(chan string)
	var wg sync.WaitGroup
	sem := make(chan struct{}, 8) // Ограничиваем количество параллельных запросов до 8
```

- **Аргументы**:
  - `ctx context.Context`: Контекст, позволяющий управлять отменой выполнения программы. Если приходит сигнал о завершении, все действия прекращаются.
  - `urls chan string`: Канал, из которого будут поступать URL-адреса для запроса.

- **Локальные переменные**:
  - `out`: Канал, в который будет отправляться результат выполнения (тело страницы).
  - `wg`: Объект `sync.WaitGroup` используется для ожидания завершения всех горутин.
  - `sem`: Буферизированный канал, который реализует семафор для ограничения числа одновременно работающих горутин. В нашем случае, не более 8 запросов будет выполняться одновременно.

#### Основной цикл:
```go
	for url := range urls {
		select {
		case <-ctx.Done():
			fmt.Println("Crawling stopped.")
			close(out)
			return out
		default:
			wg.Add(1)
			sem <- struct{}{}
			go func(url string) {
				defer wg.Done()
				defer func() { <-sem }()

				resp, err := http.Get(url)
				if err != nil {
					fmt.Println("Error fetching URL:", err)
					return
				}
				defer resp.Body.Close()

				body, err := io.ReadAll(resp.Body)
				if err != nil {
					fmt.Println("Error reading response:", err)
					return
				}
				out <- string(body)
			}(url)
		}
	}
```

- **Цикл по URL'ам**: Мы проходим по URL-адресам из канала `urls`. На каждой итерации:
  - Проверяем, не завершился ли контекст через `ctx.Done()`. Если завершился, краулер прекращает работу, выводит сообщение и закрывает выходной канал `out`.
  - Если контекст активен, добавляем одну горутину в группу ожидания `wg.Add(1)` и занимаем слот в семафоре `sem <- struct{}{}`.
  - Каждая горутина выполняет HTTP-запрос по URL:
    - `http.Get(url)`: Выполняем GET-запрос по указанному URL.
    - Если запрос успешен, читаем тело ответа `io.ReadAll(resp.Body)` и отправляем его в выходной канал `out`.
    - После завершения работы горутина освобождает слот в семафоре `defer func() { <-sem }()`, чтобы дать возможность запуститься следующей горутине.

#### Завершение работы:
```go
	go func() {
		wg.Wait()
		close(out)
	}()
```
- Эта горутина следит за завершением всех горутин с помощью `wg.Wait()`. После того, как все они завершатся, канал `out` будет закрыт, сигнализируя о завершении работы краулера.

### Функция `main`

```go
func main() {
	urls := make(chan string, 5)
	ctx, cancel := context.WithCancel(context.Background())
```

- **Создание канала `urls`**: Этот канал будет использоваться для передачи URL-адресов краулеру.
- **Создание контекста с возможностью отмены**: Контекст позволяет завершить краулер по сигналу от пользователя. Функция `cancel` позволяет отменить выполнение.

#### Обработчик для `Ctrl+C`:

```go
	go func() {
		c := make(chan os.Signal, 1)
		signal.Notify(c, os.Interrupt)
		<-c
		cancel()
	}()
```

- Эта горутина отслеживает сигнал прерывания (`Ctrl+C`) через канал `os.Signal`. Когда сигнал поступает, вызывается `cancel()`, что завершает работу краулера.

#### Заполнение канала URL-адресами:

```go
	urlList := []string{
		"https://example.com",
		"https://golang.org",
	}

	go func() {
		for _, url := range urlList {
			urls <- url
		}
		close(urls)
	}()
```

- Массив `urlList` содержит URL-адреса, которые нужно обрабатывать. В этой горутине по очереди отправляем все URL-адреса в канал `urls` и закрываем его по завершению.

#### Чтение результатов:

```go
	for body := range crawlWeb(ctx, urls) {
		fmt.Println("Page body:", body[:60])
	}
```

- Вызов функции `crawlWeb(ctx, urls)` запускает краулер, который возвращает канал с результатами (`out`).
- Цикл читает данные из этого канала и выводит первые 60 символов тела каждой страницы на консоль.

### Общая логика работы:
1. Краулер получает список URL-адресов через канал `urls`.
2. Одновременно запускается не более 8 горутин для выполнения запросов по каждому URL.
3. Если поступает сигнал `Ctrl+C`, краулер завершает работу.
4. После завершения всех запросов канал с результатами закрывается, и программа завершает работу.

### Описание работы тестов
В файле `crawlweb_test.go` находятся два теста для функции `crawlWeb`, которая выполняет параллельные HTTP-запросы. Оба теста проверяют разные аспекты работы краулера: корректную загрузку страниц и корректное завершение работы при отмене через сигнал.

### Тест 1: `TestCrawlWeb`

```go
func TestCrawlWeb(t *testing.T) {
	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	defer cancel()

	urls := make(chan string, 3)
	urls <- "https://example.com"
	urls <- "https://golang.org"
	close(urls)

	results := crawlWeb(ctx, urls)

	count := 0
	for range results {
		count++
	}

	if count < 2 {
		t.Errorf("Expected at least 2 pages to be crawled, got %d", count)
	}
}
```

- **Описание**:
  Этот тест проверяет, что функция `crawlWeb` корректно загружает страницы и возвращает результат в канал.
  
- **Контекст с тайм-аутом**:
  `context.WithTimeout` создаёт контекст, который автоматически отменяется через 5 секунд. Это гарантирует, что тест не будет зависать бесконечно. `defer cancel()` освобождает ресурсы по завершению теста.

- **Канал URL'ов**:
  Создаётся канал `urls` и в него отправляются два URL для обработки. Затем канал закрывается, чтобы сигнализировать, что все URL уже отправлены.

- **Запуск `crawlWeb`**:
  Вызывается функция `crawlWeb`, которая возвращает канал с результатами загрузки страниц.

- **Проверка результата**:
  Мы подсчитываем количество полученных результатов через цикл по каналу `results`. Ожидаем, что будет загружено как минимум 2 страницы. Если количество результатов меньше 2, тест завершится с ошибкой.

### Тест 2: `TestCrawlWebCancel`

```go
func TestCrawlWebCancel(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	urls := make(chan string, 5)

	go func() {
		urls <- "https://example.com"
		time.Sleep(500 * time.Millisecond)
		cancel() // Прерываем выполнение
		close(urls)
	}()

	results := crawlWeb(ctx, urls)
	select {
	case <-results:
		// Ожидаем, что результат придет до того, как тест завершится
	case <-time.After(3 * time.Second):
		t.Errorf("Test timed out, cancellation may not be working correctly")
	}
}
```

- **Описание**:
  Этот тест проверяет, что краулер корректно останавливает свою работу при отмене через сигнал `cancel()`.

- **Контекст с отменой**:
  Используется `context.WithCancel`, который позволяет отменить выполнение в любой момент через вызов функции `cancel()`.

- **Асинхронная загрузка URL**:
  В отдельной горутине мы отправляем один URL в канал, а затем через 500 миллисекунд вызываем `cancel()`, что прерывает выполнение краулера. После этого канал URL'ов закрывается.

- **Проверка отмены**:
  Мы запускаем `crawlWeb` и используем конструкцию `select`, чтобы проверить, завершится ли канал `results` до того, как истечет тайм-аут в 3 секунды. Если за это время ничего не произойдёт, тест завершится с ошибкой. Это проверяет корректную обработку отмены.

### Общая логика работы:

- **`TestCrawlWeb`** проверяет, что функция `crawlWeb` работает корректно и возвращает ожидаемые результаты, когда URL'ы успешно обрабатываются.
- **`TestCrawlWebCancel`** проверяет, что функция может корректно завершить работу при получении сигнала отмены.

Оба теста используются для проверки разных аспектов поведения краулера, чтобы гарантировать корректное выполнение и graceful shutdown.